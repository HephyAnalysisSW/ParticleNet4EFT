sbatch_args:
  --nodes: '1'
  --partition: g
  --gres: gpu:1
  --cpus-per-task: '6'
  --mem-per-gpu: 40G
  --qos: short
  --time: 08:00:00
  -D: /users/oskar.rothbacher/CMS/ParticleNet4EFT/


# these are the same for all scripts
train_options_const:
  --network-config:
    - 'TopDecay/networks/ParticleNet/test_pnet.py'
  --data-config:
    - 'TopDecay/data/eflow_particles_delphes_globals_ctWRe_weighted.yaml'
  --data-train:
    - >-
        /scratch-cbe/users/robert.schoefbeck/HadronicSMEFT/postprocessed/gen/v6/tschRefPointNoWidthRW/tschRefPointNoWidthRW_0?.root
        /scratch-cbe/users/robert.schoefbeck/HadronicSMEFT/postprocessed/gen/v6/tschRefPointNoWidthRW/tschRefPointNoWidthRW_[1-7]?.root
  --data-test:
    - /scratch-cbe/users/robert.schoefbeck/HadronicSMEFT/postprocessed/gen/v6/tschRefPointNoWidthRW/tschRefPointNoWidthRW_[8-9]?.root
  --regression-mode:
    - ''
  --optimizer:
    - ranger
  --start-lr:
    - 1e-4
  --lr-scheduler:
    - none
  --weighting:
    - ''
  --gpus:
    - 0
  --fetch-by-files:
    - ''
  --fetch-step:
    - 10
  --num-workers:
    - 3
  --batch-size:
    - 100
  

# these vary by script   
train_options_var:

  # any occurence of 'auto' will be replaced by network options
  --model-prefix:
    - models/pre_train_fc_pnet/v01/model    
    - models/pre_train_fc_pnet/v01/model    
    - models/pre_train_fc_pnet/v01/model    

  --num-epochs:
    - 400
    - 430
    - 460

  --load-epoch:
    - null
    - 399
    - 429

  --tensorboard:
    - runs/pre_train_fc_pnet/v01/pre_train_globals
    - runs/pre_train_fc_pnet/v01/pre_train_particles
    - runs/pre_train_fc_pnet/v01/finalize

network_options:

  conv_params:
    - '[(16, (64, 64, 64)), (16, (128, 128, 128)), (16, (256, 256, 256))]'
    - '[(16, (64, 64, 64)), (16, (128, 128, 128)), (16, (256, 256, 256))]'
    - '[(16, (64, 64, 64)), (16, (128, 128, 128)), (16, (256, 256, 256))]'

  pnet_fc_params: 
    - '[(128, 0.0)]'
    - '[(128, 0.0)]'
    - '[(128, 0.0)]'

  freeze_pnet:
    - 'True'
    - 'False'
    - 'False'

  freeze_global_fc:
    - 'False'
    - 'True'
    - 'False'


  globals_fc_params:
    - '[(256, 0.0), (256, 0.0)]'
    - '[(256, 0.0), (256, 0.0)]'
    - '[(256, 0.0), (256, 0.0)]'

  joined_fc_params:
    - '[(256, 0.0), (128, 0.0)]'
    - '[(256, 0.0), (128, 0.0)]'
    - '[(256, 0.0), (128, 0.0)]'

# script names (eg when generating scripts for continued training with the same model prefix)
# for now scripts will be submitted in alphabetical order, so choose names appropriatley if needed
script_name:
  - 0_pre_train_globals
  - 1_pre_train_particles
  - 2_finalize